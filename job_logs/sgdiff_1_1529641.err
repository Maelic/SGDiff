+ cd /gpfswork/rech/gtb/ukj95zg/SGDiff
+ srun python trainer.py --resume /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt --base config_vg.yaml -t --gpus 0,1
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Restoring states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 23
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  f"DataModule.{name} has already been called, so it will not be called again. "
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2023-11-22T14-04-33_config_vg.
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Set SLURM handle signals.
Set SLURM handle signals.
Restored all states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 395 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 67.7 M
3 | cond_stage_model  | CGIPModel        | 13.2 M
-------------------------------------------------------
395 M     Trainable params
81.0 M    Non-trainable params
476 M     Total params
1,906.921 Total estimated model params size (MB)
Global seed set to 23
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/lr_monitor.py:116: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  RuntimeWarning,
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py:103: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5
  "The signature of `Callback.on_train_epoch_end` has changed in v1.3."
Average Epoch time: 2105.82 seconds
Average Peak memory 27999.57MiB
Epoch 199, global step 391199: val/loss_simple_ema reached 0.16297 (best 0.16297), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000199.ckpt" as top 3
Average Epoch time: 2101.95 seconds
Average Peak memory 15074.61MiB
Epoch 200, global step 393155: val/loss_simple_ema reached 0.16366 (best 0.16297), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000200.ckpt" as top 3
Average Epoch time: 2101.18 seconds
Average Peak memory 15074.61MiB
Epoch 201, global step 395111: val/loss_simple_ema reached 0.15715 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000201.ckpt" as top 3
Average Epoch time: 2101.35 seconds
Average Peak memory 15076.36MiB
Epoch 202, global step 397067: val/loss_simple_ema reached 0.16150 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000202.ckpt" as top 3
Average Epoch time: 2100.82 seconds
Average Peak memory 15074.11MiB
Epoch 203, global step 399023: val/loss_simple_ema reached 0.16293 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000203.ckpt" as top 3
Average Epoch time: 2102.21 seconds
Average Peak memory 15075.86MiB
Epoch 204, global step 400979: val/loss_simple_ema reached 0.15903 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000204.ckpt" as top 3
Average Epoch time: 2102.68 seconds
Average Peak memory 15073.36MiB
Epoch 205, global step 402935: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.42 seconds
Average Peak memory 15074.61MiB
Epoch 206, global step 404891: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.45 seconds
Average Peak memory 15074.36MiB
Epoch 207, global step 406847: val/loss_simple_ema reached 0.15802 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000207.ckpt" as top 3
Average Epoch time: 2104.03 seconds
Average Peak memory 15076.11MiB
Epoch 208, global step 408803: val/loss_simple_ema was not in top 3
Average Epoch time: 2103.03 seconds
Average Peak memory 15073.86MiB
Epoch 209, global step 410759: val/loss_simple_ema reached 0.15889 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000209.ckpt" as top 3
Average Epoch time: 2100.64 seconds
Average Peak memory 15078.36MiB
Epoch 210, global step 412715: val/loss_simple_ema reached 0.15867 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000210.ckpt" as top 3
Average Epoch time: 2101.63 seconds
Average Peak memory 15075.86MiB
Epoch 211, global step 414671: val/loss_simple_ema was not in top 3
Average Epoch time: 2103.01 seconds
Average Peak memory 15078.61MiB
Epoch 212, global step 416627: val/loss_simple_ema was not in top 3
Average Epoch time: 2096.49 seconds
Average Peak memory 15075.86MiB
Epoch 213, global step 418583: val/loss_simple_ema was not in top 3
Average Epoch time: 2090.07 seconds
Average Peak memory 15078.61MiB
Epoch 214, global step 420539: val/loss_simple_ema was not in top 3
Average Epoch time: 2092.53 seconds
Average Peak memory 15075.86MiB
Epoch 215, global step 422495: val/loss_simple_ema was not in top 3
Average Epoch time: 2092.83 seconds
Average Peak memory 15078.61MiB
Epoch 216, global step 424451: val/loss_simple_ema was not in top 3
Average Epoch time: 2093.77 seconds
Average Peak memory 15075.86MiB
Epoch 217, global step 426407: val/loss_simple_ema reached 0.15838 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000217.ckpt" as top 3
Average Epoch time: 2095.85 seconds
Average Peak memory 15078.61MiB
Epoch 218, global step 428363: val/loss_simple_ema was not in top 3
Average Epoch time: 2095.12 seconds
Average Peak memory 15075.86MiB
Epoch 219, global step 430319: val/loss_simple_ema was not in top 3
Average Epoch time: 2095.46 seconds
Average Peak memory 15078.61MiB
Epoch 220, global step 432275: val/loss_simple_ema reached 0.15785 (best 0.15715), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000220.ckpt" as top 3
Average Epoch time: 2095.23 seconds
Average Peak memory 15075.86MiB
Epoch 221, global step 434231: val/loss_simple_ema was not in top 3
Average Epoch time: 2095.25 seconds
Average Peak memory 15078.61MiB
Epoch 222, global step 436187: val/loss_simple_ema was not in top 3
Average Epoch time: 2092.94 seconds
Average Peak memory 15075.86MiB
Epoch 223, global step 438143: val/loss_simple_ema was not in top 3
Average Epoch time: 2090.79 seconds
Average Peak memory 15078.61MiB
Epoch 224, global step 440099: val/loss_simple_ema was not in top 3
Average Epoch time: 2093.24 seconds
Average Peak memory 15075.86MiB
Epoch 225, global step 442055: val/loss_simple_ema reached 0.15491 (best 0.15491), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000225.ckpt" as top 3
Average Epoch time: 2092.50 seconds
Average Peak memory 15078.61MiB
Epoch 226, global step 444011: val/loss_simple_ema was not in top 3
Average Epoch time: 2093.21 seconds
Average Peak memory 15075.86MiB
Epoch 227, global step 445967: val/loss_simple_ema was not in top 3
Average Epoch time: 2092.43 seconds
Average Peak memory 15078.61MiB
Epoch 228, global step 447923: val/loss_simple_ema was not in top 3
Average Epoch time: 2092.16 seconds
Average Peak memory 15075.86MiB
Epoch 229, global step 449879: val/loss_simple_ema was not in top 3
Average Epoch time: 2091.71 seconds
Average Peak memory 15078.61MiB
Epoch 230, global step 451835: val/loss_simple_ema was not in top 3
Average Epoch time: 2095.03 seconds
Average Peak memory 15075.86MiB
Epoch 231, global step 453791: val/loss_simple_ema was not in top 3
slurmstepd: error: *** STEP 1529641.0 ON r6i4n4 CANCELLED AT 2023-12-04T13:24:07 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 1529641 ON r6i4n4 CANCELLED AT 2023-12-04T13:24:07 DUE TO TIME LIMIT ***
