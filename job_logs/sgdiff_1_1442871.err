+ cd /gpfswork/rech/gtb/ukj95zg/SGDiff
+ srun python trainer.py --resume /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt --base config_vg.yaml -t --gpus 0,1
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Restoring states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt
Global seed set to 23
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 23
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  f"DataModule.{name} has already been called, so it will not be called again. "
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2023-11-22T14-04-33_config_vg.
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Set SLURM handle signals.
Set SLURM handle signals.
Restored all states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 395 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 67.7 M
3 | cond_stage_model  | CGIPModel        | 13.2 M
-------------------------------------------------------
395 M     Trainable params
81.0 M    Non-trainable params
476 M     Total params
1,906.921 Total estimated model params size (MB)
Global seed set to 23
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/lr_monitor.py:116: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  RuntimeWarning,
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py:103: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5
  "The signature of `Callback.on_train_epoch_end` has changed in v1.3."
Average Epoch time: 2142.69 seconds
Average Peak memory 27998.67MiB
Epoch 66, global step 131051: val/loss_simple_ema reached 0.16569 (best 0.16569), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000066.ckpt" as top 3
Average Epoch time: 2112.87 seconds
Average Peak memory 15074.96MiB
Epoch 67, global step 133007: val/loss_simple_ema reached 0.16636 (best 0.16569), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000067.ckpt" as top 3
Average Epoch time: 2130.42 seconds
Average Peak memory 15077.21MiB
Epoch 68, global step 134963: val/loss_simple_ema reached 0.15966 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000068.ckpt" as top 3
Average Epoch time: 2146.24 seconds
Average Peak memory 15087.21MiB
Epoch 69, global step 136919: val/loss_simple_ema reached 0.16406 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000069.ckpt" as top 3
Average Epoch time: 2130.61 seconds
Average Peak memory 15087.21MiB
Epoch 70, global step 138875: val/loss_simple_ema reached 0.16538 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000070.ckpt" as top 3
Average Epoch time: 2118.83 seconds
Average Peak memory 15079.21MiB
Epoch 71, global step 140831: val/loss_simple_ema reached 0.16141 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000071.ckpt" as top 3
Average Epoch time: 2097.73 seconds
Average Peak memory 15082.21MiB
Epoch 72, global step 142787: val/loss_simple_ema was not in top 3
Average Epoch time: 2116.30 seconds
Average Peak memory 15082.71MiB
Epoch 73, global step 144743: val/loss_simple_ema was not in top 3
Average Epoch time: 2138.27 seconds
Average Peak memory 15088.71MiB
Epoch 74, global step 146699: val/loss_simple_ema reached 0.16028 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000074.ckpt" as top 3
Average Epoch time: 2097.60 seconds
Average Peak memory 15083.96MiB
Epoch 75, global step 148655: val/loss_simple_ema was not in top 3
Average Epoch time: 2105.32 seconds
Average Peak memory 15084.71MiB
Epoch 76, global step 150611: val/loss_simple_ema reached 0.16110 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000076.ckpt" as top 3
Average Epoch time: 2100.43 seconds
Average Peak memory 15086.96MiB
Epoch 77, global step 152567: val/loss_simple_ema reached 0.16083 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000077.ckpt" as top 3
Average Epoch time: 2103.14 seconds
Average Peak memory 15088.21MiB
Epoch 78, global step 154523: val/loss_simple_ema was not in top 3
Average Epoch time: 2101.45 seconds
Average Peak memory 15087.46MiB
Epoch 79, global step 156479: val/loss_simple_ema was not in top 3
Average Epoch time: 2101.63 seconds
Average Peak memory 15083.21MiB
Epoch 80, global step 158435: val/loss_simple_ema was not in top 3
Average Epoch time: 2098.34 seconds
Average Peak memory 15086.21MiB
Epoch 81, global step 160391: val/loss_simple_ema was not in top 3
Average Epoch time: 2114.01 seconds
Average Peak memory 15084.96MiB
Epoch 82, global step 162347: val/loss_simple_ema was not in top 3
Average Epoch time: 2113.28 seconds
Average Peak memory 15088.46MiB
Epoch 83, global step 164303: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.93 seconds
Average Peak memory 15085.96MiB
Epoch 84, global step 166259: val/loss_simple_ema reached 0.16026 (best 0.15966), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000084.ckpt" as top 3
Average Epoch time: 2161.21 seconds
Average Peak memory 15084.96MiB
Epoch 85, global step 168215: val/loss_simple_ema was not in top 3
Average Epoch time: 2101.78 seconds
Average Peak memory 15083.21MiB
Epoch 86, global step 170171: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.44 seconds
Average Peak memory 15086.96MiB
Epoch 87, global step 172127: val/loss_simple_ema reached 0.15963 (best 0.15963), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000087.ckpt" as top 3
Average Epoch time: 2109.19 seconds
Average Peak memory 15089.21MiB
Epoch 88, global step 174083: val/loss_simple_ema was not in top 3
Average Epoch time: 2137.04 seconds
Average Peak memory 15088.71MiB
Epoch 89, global step 176039: val/loss_simple_ema was not in top 3
Average Epoch time: 2104.61 seconds
Average Peak memory 15087.71MiB
Epoch 90, global step 177995: val/loss_simple_ema was not in top 3
Average Epoch time: 2106.53 seconds
Average Peak memory 15087.96MiB
Epoch 91, global step 179951: val/loss_simple_ema reached 0.16005 (best 0.15963), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000091.ckpt" as top 3
Average Epoch time: 2100.24 seconds
Average Peak memory 15089.71MiB
Epoch 92, global step 181907: val/loss_simple_ema reached 0.15647 (best 0.15647), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000092.ckpt" as top 3
Average Epoch time: 2115.94 seconds
Average Peak memory 15092.71MiB
Epoch 93, global step 183863: val/loss_simple_ema was not in top 3
Average Epoch time: 2119.99 seconds
Average Peak memory 15093.21MiB
Epoch 94, global step 185819: val/loss_simple_ema was not in top 3
Average Epoch time: 2130.11 seconds
Average Peak memory 15092.71MiB
Epoch 95, global step 187775: val/loss_simple_ema was not in top 3
Average Epoch time: 2154.70 seconds
Average Peak memory 15094.71MiB
Epoch 96, global step 189731: val/loss_simple_ema was not in top 3
Average Epoch time: 2127.67 seconds
Average Peak memory 15090.21MiB
Epoch 97, global step 191687: val/loss_simple_ema was not in top 3
Average Epoch time: 2131.46 seconds
Average Peak memory 15091.96MiB
Epoch 98, global step 193643: val/loss_simple_ema was not in top 3
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** STEP 1442871.0 ON r8i3n0 CANCELLED AT 2023-11-30T05:49:24 DUE TO TIME LIMIT ***
slurmstepd: error: *** JOB 1442871 ON r8i3n0 CANCELLED AT 2023-11-30T05:49:24 DUE TO TIME LIMIT ***
bypassing sigterm
bypassing sigterm
