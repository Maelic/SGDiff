+ cd /gpfswork/rech/gtb/ukj95zg/SGDiff
+ srun python trainer.py --resume /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt --base config_vg.yaml -t --gpus 0,1
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Restoring states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 23
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  f"DataModule.{name} has already been called, so it will not be called again. "
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2023-11-22T14-04-33_config_vg.
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Set SLURM handle signals.
Set SLURM handle signals.
Restored all states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 395 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 67.7 M
3 | cond_stage_model  | CGIPModel        | 13.2 M
-------------------------------------------------------
395 M     Trainable params
81.0 M    Non-trainable params
476 M     Total params
1,906.921 Total estimated model params size (MB)
Global seed set to 23
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/lr_monitor.py:116: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  RuntimeWarning,
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py:103: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5
  "The signature of `Callback.on_train_epoch_end` has changed in v1.3."
Average Epoch time: 2123.02 seconds
Average Peak memory 27999.57MiB
Epoch 298, global step 584843: val/loss_simple_ema reached 0.16352 (best 0.16352), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000298.ckpt" as top 3
Average Epoch time: 2112.30 seconds
Average Peak memory 15074.60MiB
Epoch 299, global step 586799: val/loss_simple_ema reached 0.16422 (best 0.16352), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000299.ckpt" as top 3
Average Epoch time: 2436.57 seconds
Average Peak memory 15074.61MiB
Epoch 300, global step 588755: val/loss_simple_ema reached 0.15770 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000300.ckpt" as top 3
Average Epoch time: 2114.46 seconds
Average Peak memory 15076.36MiB
Epoch 301, global step 590711: val/loss_simple_ema reached 0.16211 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000301.ckpt" as top 3
Average Epoch time: 2115.26 seconds
Average Peak memory 15074.11MiB
Epoch 302, global step 592667: val/loss_simple_ema was not in top 3
Average Epoch time: 2117.09 seconds
Average Peak memory 15075.86MiB
Epoch 303, global step 594623: val/loss_simple_ema reached 0.15965 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000303.ckpt" as top 3
Average Epoch time: 2118.53 seconds
Average Peak memory 15073.36MiB
Epoch 304, global step 596579: val/loss_simple_ema was not in top 3
Average Epoch time: 2115.95 seconds
Average Peak memory 15074.61MiB
Epoch 305, global step 598535: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.71 seconds
Average Peak memory 15074.36MiB
Epoch 306, global step 600491: val/loss_simple_ema reached 0.15869 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000306.ckpt" as top 3
Average Epoch time: 2125.23 seconds
Average Peak memory 15076.11MiB
Epoch 307, global step 602447: val/loss_simple_ema was not in top 3
Average Epoch time: 2124.14 seconds
Average Peak memory 15073.86MiB
Epoch 308, global step 604403: val/loss_simple_ema reached 0.15961 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000308.ckpt" as top 3
Average Epoch time: 2123.52 seconds
Average Peak memory 15078.36MiB
Epoch 309, global step 606359: val/loss_simple_ema reached 0.15938 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000309.ckpt" as top 3
Average Epoch time: 2128.52 seconds
Average Peak memory 15075.86MiB
Epoch 310, global step 608315: val/loss_simple_ema was not in top 3
Average Epoch time: 2127.64 seconds
Average Peak memory 15078.61MiB
Epoch 311, global step 610271: val/loss_simple_ema was not in top 3
Average Epoch time: 2127.83 seconds
Average Peak memory 15075.86MiB
Epoch 312, global step 612227: val/loss_simple_ema was not in top 3
Average Epoch time: 2126.44 seconds
Average Peak memory 15078.61MiB
Epoch 313, global step 614183: val/loss_simple_ema was not in top 3
Average Epoch time: 2126.22 seconds
Average Peak memory 15075.86MiB
Epoch 314, global step 616139: val/loss_simple_ema was not in top 3
Average Epoch time: 2123.30 seconds
Average Peak memory 15078.61MiB
Epoch 315, global step 618095: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.55 seconds
Average Peak memory 15075.86MiB
Epoch 316, global step 620051: val/loss_simple_ema reached 0.15924 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000316.ckpt" as top 3
Average Epoch time: 2128.90 seconds
Average Peak memory 15078.61MiB
Epoch 317, global step 622007: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.13 seconds
Average Peak memory 15075.86MiB
Epoch 318, global step 623963: val/loss_simple_ema was not in top 3
Average Epoch time: 2122.70 seconds
Average Peak memory 15078.61MiB
Epoch 319, global step 625919: val/loss_simple_ema reached 0.15876 (best 0.15770), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000319.ckpt" as top 3
Average Epoch time: 2124.49 seconds
Average Peak memory 15075.86MiB
Epoch 320, global step 627875: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.13 seconds
Average Peak memory 15078.61MiB
Epoch 321, global step 629831: val/loss_simple_ema was not in top 3
Average Epoch time: 2127.29 seconds
Average Peak memory 15075.86MiB
Epoch 322, global step 631787: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.74 seconds
Average Peak memory 15078.61MiB
Epoch 323, global step 633743: val/loss_simple_ema was not in top 3
Average Epoch time: 2124.61 seconds
Average Peak memory 15075.86MiB
Epoch 324, global step 635699: val/loss_simple_ema reached 0.15586 (best 0.15586), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000324.ckpt" as top 3
Average Epoch time: 2128.19 seconds
Average Peak memory 15078.61MiB
Epoch 325, global step 637655: val/loss_simple_ema was not in top 3
Average Epoch time: 2124.76 seconds
Average Peak memory 15075.86MiB
Epoch 326, global step 639611: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.80 seconds
Average Peak memory 15078.61MiB
Epoch 327, global step 641567: val/loss_simple_ema was not in top 3
Average Epoch time: 2123.53 seconds
Average Peak memory 15075.86MiB
Epoch 328, global step 643523: val/loss_simple_ema was not in top 3
Average Epoch time: 2293.70 seconds
Average Peak memory 15078.61MiB
Epoch 329, global step 645479: val/loss_simple_ema was not in top 3
Average Epoch time: 2125.24 seconds
Average Peak memory 15075.86MiB
Epoch 330, global step 647435: val/loss_simple_ema was not in top 3
slurmstepd: error: *** STEP 1590419.0 ON r7i6n1 CANCELLED AT 2023-12-07T10:03:17 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 1590419 ON r7i6n1 CANCELLED AT 2023-12-07T10:03:17 DUE TO TIME LIMIT ***
