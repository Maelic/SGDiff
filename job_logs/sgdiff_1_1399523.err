+ cd /gpfswork/rech/gtb/ukj95zg/SGDiff
+ srun python trainer.py --resume /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt --base config_vg.yaml -t --gpus 0,1
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
Restoring states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt
Global seed set to 23
Global seed set to 23
initializing ddp: GLOBAL_RANK: 0, MEMBER: 1/2
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/model_checkpoint.py:446: UserWarning: Checkpoint directory /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
Global seed set to 23
initializing ddp: GLOBAL_RANK: 1, MEMBER: 2/2
----------------------------------------------------------------------------------------------------
distributed_backend=nccl
All DDP processes registered. Starting ddp with 2 processes
----------------------------------------------------------------------------------------------------

LOCAL_RANK: 1 - CUDA_VISIBLE_DEVICES: [0,1]
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/core/datamodule.py:424: LightningDeprecationWarning: DataModule.setup has already been called, so it will not be called again. In v1.6 this behavior will change to always call DataModule.setup.
  f"DataModule.{name} has already been called, so it will not be called again. "
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0,1]
wandb: WARNING `resume` will be ignored since W&B syncing is set to `offline`. Starting a new run with run id 2023-11-22T14-04-33_config_vg.
wandb: Tracking run with wandb version 0.15.12
wandb: W&B syncing is set to `offline` in this directory.  
wandb: Run `wandb online` or set WANDB_MODE=online to enable cloud syncing.
Set SLURM handle signals.
Set SLURM handle signals.
Restored all states from the checkpoint file at /gpfswork/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/last.ckpt

  | Name              | Type             | Params
-------------------------------------------------------
0 | model             | DiffusionWrapper | 395 M 
1 | model_ema         | LitEma           | 0     
2 | first_stage_model | VQModelInterface | 67.7 M
3 | cond_stage_model  | CGIPModel        | 13.2 M
-------------------------------------------------------
395 M     Trainable params
81.0 M    Non-trainable params
476 M     Total params
1,906.921 Total estimated model params size (MB)
Global seed set to 23
Global seed set to 23
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/callbacks/lr_monitor.py:116: RuntimeWarning: You are using `LearningRateMonitor` callback with models that have no learning rate schedulers. Please see documentation for `configure_optimizers` method.
  RuntimeWarning,
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
[W reducer.cpp:1251] Warning: find_unused_parameters=True was specified in DDP constructor, but did not find any unused parameters in the forward pass. This flag results in an extra traversal of the autograd graph every iteration,  which can adversely affect performance. If your model indeed never has any unused parameters in the forward pass, consider turning this flag off. Note that this warning may be a false positive if your model has flow control causing later iterations to have unused parameters. (function operator())
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/torch/autograd/__init__.py:175: UserWarning: Grad strides do not match bucket view strides. This may indicate grad was not created according to the gradient layout contract, or that the param's strides changed since DDP was constructed.  This is not an error, but may impair performance.
grad.sizes() = [256, 256, 1, 1], strides() = [256, 1, 256, 256]
bucket_view.sizes() = [256, 256, 1, 1], strides() = [256, 1, 1, 1] (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484809535/work/torch/csrc/distributed/c10d/reducer.cpp:312.)
  allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass
/gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/trainer/callback_hook.py:103: LightningDeprecationWarning: The signature of `Callback.on_train_epoch_end` has changed in v1.3. `outputs` parameter has been removed. Support for the old signature will be removed in v1.5
  "The signature of `Callback.on_train_epoch_end` has changed in v1.3."
Average Epoch time: 2104.65 seconds
Average Peak memory 27998.67MiB
Epoch 33, global step 66503: val/loss_simple_ema reached 0.16833 (best 0.16833), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000033.ckpt" as top 3
Average Epoch time: 2098.64 seconds
Average Peak memory 15074.96MiB
Epoch 34, global step 68459: val/loss_simple_ema reached 0.16895 (best 0.16833), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000034.ckpt" as top 3
Average Epoch time: 2102.59 seconds
Average Peak memory 15077.21MiB
Epoch 35, global step 70415: val/loss_simple_ema reached 0.16207 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000035.ckpt" as top 3
Average Epoch time: 2102.92 seconds
Average Peak memory 15087.21MiB
Epoch 36, global step 72371: val/loss_simple_ema reached 0.16650 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000036.ckpt" as top 3
Average Epoch time: 2102.64 seconds
Average Peak memory 15087.21MiB
Epoch 37, global step 74327: val/loss_simple_ema reached 0.16774 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000037.ckpt" as top 3
Average Epoch time: 2103.07 seconds
Average Peak memory 15079.21MiB
Epoch 38, global step 76283: val/loss_simple_ema reached 0.16363 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000038.ckpt" as top 3
Average Epoch time: 2101.23 seconds
Average Peak memory 15082.21MiB
Epoch 39, global step 78239: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.20 seconds
Average Peak memory 15082.71MiB
Epoch 40, global step 80195: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.54 seconds
Average Peak memory 15088.71MiB
Epoch 41, global step 82151: val/loss_simple_ema reached 0.16240 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000041.ckpt" as top 3
Average Epoch time: 2102.87 seconds
Average Peak memory 15083.96MiB
Epoch 42, global step 84107: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.06 seconds
Average Peak memory 15084.71MiB
Epoch 43, global step 86063: val/loss_simple_ema reached 0.16312 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000043.ckpt" as top 3
Average Epoch time: 2101.88 seconds
Average Peak memory 15086.96MiB
Epoch 44, global step 88019: val/loss_simple_ema reached 0.16282 (best 0.16207), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000044.ckpt" as top 3
Average Epoch time: 2102.11 seconds
Average Peak memory 15088.21MiB
Epoch 45, global step 89975: val/loss_simple_ema was not in top 3
Average Epoch time: 2099.40 seconds
Average Peak memory 15087.46MiB
Epoch 46, global step 91931: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.68 seconds
Average Peak memory 15083.21MiB
Epoch 47, global step 93887: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.09 seconds
Average Peak memory 15086.21MiB
Epoch 48, global step 95843: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.41 seconds
Average Peak memory 15084.96MiB
Epoch 49, global step 97799: val/loss_simple_ema was not in top 3
Average Epoch time: 2101.46 seconds
Average Peak memory 15088.46MiB
Epoch 50, global step 99755: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.74 seconds
Average Peak memory 15085.96MiB
Epoch 51, global step 101711: val/loss_simple_ema reached 0.16196 (best 0.16196), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000051.ckpt" as top 3
Average Epoch time: 2098.42 seconds
Average Peak memory 15084.96MiB
Epoch 52, global step 103667: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.37 seconds
Average Peak memory 15083.21MiB
Epoch 53, global step 105623: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.50 seconds
Average Peak memory 15086.96MiB
Epoch 54, global step 107579: val/loss_simple_ema reached 0.16126 (best 0.16126), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000054.ckpt" as top 3
Average Epoch time: 2101.27 seconds
Average Peak memory 15089.21MiB
Epoch 55, global step 109535: val/loss_simple_ema was not in top 3
Average Epoch time: 2098.31 seconds
Average Peak memory 15088.71MiB
Epoch 56, global step 111491: val/loss_simple_ema was not in top 3
Average Epoch time: 2103.01 seconds
Average Peak memory 15087.71MiB
Epoch 57, global step 113447: val/loss_simple_ema was not in top 3
Average Epoch time: 2103.05 seconds
Average Peak memory 15087.96MiB
Epoch 58, global step 115403: val/loss_simple_ema reached 0.16158 (best 0.16126), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000058.ckpt" as top 3
Average Epoch time: 2102.62 seconds
Average Peak memory 15089.71MiB
Epoch 59, global step 117359: val/loss_simple_ema reached 0.15789 (best 0.15789), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000059.ckpt" as top 3
Average Epoch time: 2102.43 seconds
Average Peak memory 15092.71MiB
Epoch 60, global step 119315: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.44 seconds
Average Peak memory 15093.21MiB
Epoch 61, global step 121271: val/loss_simple_ema was not in top 3
Average Epoch time: 2102.48 seconds
Average Peak memory 15092.71MiB
Epoch 62, global step 123227: val/loss_simple_ema reached 0.16116 (best 0.15789), saving model to "/gpfsdswork/projects/rech/gtb/ukj95zg/SGDiff/logs/2023-11-22T14-04-33_config_vg/checkpoints/epoch=000062.ckpt" as top 3
Average Epoch time: 2102.54 seconds
Average Peak memory 15094.71MiB
Epoch 63, global step 125183: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.35 seconds
Average Peak memory 15090.21MiB
Epoch 64, global step 127139: val/loss_simple_ema was not in top 3
Average Epoch time: 2100.07 seconds
Average Peak memory 15091.96MiB
Epoch 65, global step 129095: val/loss_simple_ema was not in top 3
slurmstepd: error: *** STEP 1399523.0 ON r7i4n3 CANCELLED AT 2023-11-28T07:20:22 DUE TO TIME LIMIT ***
srun: Job step aborted: Waiting up to 62 seconds for job step to finish.
slurmstepd: error: *** JOB 1399523 ON r7i4n3 CANCELLED AT 2023-11-28T07:20:22 DUE TO TIME LIMIT ***
bypassing sigterm
