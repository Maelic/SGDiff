{'benchmark': True}
Running on GPUs 0,1
LatentDiffusion: Running in eps-prediction mode
DiffusionWrapper has 395.77 M params.
Keeping EMAs of 630.
making attention of type 'vanilla' with 512 in_channels
making attention of type 'vanilla' with 512 in_channels
making attention of type 'vanilla' with 512 in_channels
Working with z of shape (1, 4, 32, 32) = 4096 dimensions.
making attention of type 'vanilla' with 512 in_channels
making attention of type 'vanilla' with 512 in_channels
making attention of type 'vanilla' with 512 in_channels
making attention of type 'vanilla' with 512 in_channels
Restored from pretrained/vq-f8-model.ckpt with 0 missing and 49 unexpected keys
Restored from pretrained/sip_vg.pt with 0 missing and 308 unexpected keys
Monitoring val/loss_simple_ema as checkpoint metric.
Merged modelckpt-cfg: 
{'target': 'pytorch_lightning.callbacks.ModelCheckpoint', 'params': {'dirpath': 'logs/2023-11-21T18-29-40_config_vg/checkpoints', 'filename': '{epoch:06}', 'verbose': True, 'save_last': True, 'monitor': 'val/loss_simple_ema', 'save_top_k': 3}}
#### Data #####
train, VGTrain, 62565
validation, VGTrain, 5062
accumulate_grad_batches = 1
Setting learning rate to 3.20e-05 = 1 (accumulate_grad_batches) * 2 (num_gpus) * 16 (batchsize) * 1.00e-06 (base_lr)
Problem at: /gpfsdswork/projects/rech/gtb/ukj95zg/miniconda3/envs/sgdiff/lib/python3.7/site-packages/pytorch_lightning/loggers/wandb.py 193 experiment
Summoning checkpoint.

